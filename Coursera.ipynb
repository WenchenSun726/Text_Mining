{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling -  Coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.0 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: pytest in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (3.5.1)\n",
      "Requirement already satisfied: future in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.17.1)\n",
      "Requirement already satisfied: numexpr in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.6.5)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.10)\n",
      "Requirement already satisfied: funcy in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.11)\n",
      "Requirement already satisfied: joblib>=0.8.4 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.14.3)\n",
      "Requirement already satisfied: wheel>=0.23.0 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.31.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.23.0)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (1.5.3)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (1.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (39.1.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (18.1.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (4.1.0)\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (0.6.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (0.3.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anaconda3\\lib\\site-packages (from jinja2>=2.7.2->pyLDAvis) (1.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\anaconda3\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2018.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\anaconda3\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2.7.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "!{sys.executable} -m pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html.parser\n",
      "Requirement already satisfied: ply in c:\\anaconda3\\lib\\site-packages (from html.parser) (3.11)\n",
      "Installing collected packages: html.parser\n",
      "Successfully installed html.parser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pattern3 in c:\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: pdfminer.six in c:\\anaconda3\\lib\\site-packages (from pattern3) (20181108)\n",
      "Requirement already satisfied: pdfminer3k in c:\\anaconda3\\lib\\site-packages (from pattern3) (1.3.1)\n",
      "Requirement already satisfied: cherrypy in c:\\anaconda3\\lib\\site-packages (from pattern3) (18.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda3\\lib\\site-packages (from pattern3) (4.6.0)\n",
      "Requirement already satisfied: feedparser in c:\\anaconda3\\lib\\site-packages (from pattern3) (5.2.1)\n",
      "Requirement already satisfied: docx in c:\\anaconda3\\lib\\site-packages (from pattern3) (0.2.4)\n",
      "Requirement already satisfied: simplejson in c:\\anaconda3\\lib\\site-packages (from pattern3) (3.16.0)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern3) (1.11.0)\n",
      "Requirement already satisfied: pycryptodome in c:\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern3) (3.8.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern3) (1.5.10)\n",
      "Requirement already satisfied: pytest>=2.0 in c:\\anaconda3\\lib\\site-packages (from pdfminer3k->pattern3) (3.5.1)\n",
      "Requirement already satisfied: ply>=3.4 in c:\\anaconda3\\lib\\site-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: zc.lockfile in c:\\anaconda3\\lib\\site-packages (from cherrypy->pattern3) (1.4)\n",
      "Requirement already satisfied: portend>=2.1.1 in c:\\anaconda3\\lib\\site-packages (from cherrypy->pattern3) (2.3)\n",
      "Requirement already satisfied: cheroot>=6.2.4 in c:\\anaconda3\\lib\\site-packages (from cherrypy->pattern3) (6.5.4)\n",
      "Requirement already satisfied: more-itertools in c:\\anaconda3\\lib\\site-packages (from cherrypy->pattern3) (4.1.0)\n",
      "Requirement already satisfied: pywin32; sys_platform == \"win32\" in c:\\anaconda3\\lib\\site-packages (from cherrypy->pattern3) (223)\n",
      "Requirement already satisfied: lxml in c:\\anaconda3\\lib\\site-packages (from docx->pattern3) (4.2.1)\n",
      "Requirement already satisfied: Pillow>=2.0 in c:\\anaconda3\\lib\\site-packages (from docx->pattern3) (5.1.0)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k->pattern3) (1.5.3)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k->pattern3) (39.1.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k->pattern3) (18.1.0)\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in c:\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k->pattern3) (0.6.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k->pattern3) (0.3.9)\n",
      "Requirement already satisfied: tempora>=1.8 in c:\\anaconda3\\lib\\site-packages (from portend>=2.1.1->cherrypy->pattern3) (1.14)\n",
      "Requirement already satisfied: backports.functools-lru-cache in c:\\anaconda3\\lib\\site-packages (from cheroot>=6.2.4->cherrypy->pattern3) (1.5)\n",
      "Requirement already satisfied: jaraco.functools>=1.20 in c:\\anaconda3\\lib\\site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2.0)\n",
      "Requirement already satisfied: pytz in c:\\anaconda3\\lib\\site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2018.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: future in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.17.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.23.0)\n",
      "Requirement already satisfied: joblib>=0.8.4 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.13.2)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.10)\n",
      "Requirement already satisfied: pytest in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (3.5.1)\n",
      "Requirement already satisfied: funcy in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.11)\n",
      "Requirement already satisfied: numexpr in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.6.5)\n",
      "Requirement already satisfied: wheel>=0.23.0 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.14.3)\n",
      "Requirement already satisfied: scipy>=0.18.0 in c:\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\anaconda3\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2018.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\anaconda3\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anaconda3\\lib\\site-packages (from jinja2>=2.7.2->pyLDAvis) (1.0)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (1.5.3)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (1.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (39.1.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (18.1.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (4.1.0)\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (0.6.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (0.3.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Original:   [\"<p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\", \"<p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\"] \n",
      "\n",
      "Processed:  ['circus dog plisse skirt jump python large foot long', 'circus dog plisse skirt jump python large foot long']\n"
     ]
    }
   ],
   "source": [
    "path = '--'\n",
    "%run ./Text_Normalization_Function.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+\"Coursera_txt.txt\", delimiter = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove http, and @ and username\n",
    "for index, row in df.iterrows():\n",
    "    row[0] = str(row[0]).split(\" \")\n",
    "    for i in range(len(row[0])):\n",
    "        row[0][i] = \"\".join([word for word in row[0][i].split()\n",
    "                            if \"http\" not in word and \"@\" not in word and \"<\" not in word])\n",
    "    row[0] = \",\".join(row[0])\n",
    "    row[0] = str(row[0]).replace(\",\", \" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Set Up the Topic Model (LDA) ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0m</th>\n",
       "      <th>100daysofcode</th>\n",
       "      <th>100daysofmlcode</th>\n",
       "      <th>100daysofmlcodestudying</th>\n",
       "      <th>100finished</th>\n",
       "      <th>103mfor</th>\n",
       "      <th>10kwomen</th>\n",
       "      <th>1m</th>\n",
       "      <th>4th</th>\n",
       "      <th>5m</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yearup</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>york</th>\n",
       "      <th>youll</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourselfshun</th>\n",
       "      <th>yourstoryco</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0m  100daysofcode  100daysofmlcode  100daysofmlcodestudying  100finished  \\\n",
       "0   0              0                0                        0            0   \n",
       "1   0              0                0                        0            0   \n",
       "2   0              0                0                        0            0   \n",
       "3   0              1                0                        0            0   \n",
       "4   0              1                0                        0            0   \n",
       "\n",
       "   103mfor  10kwomen  1m  4th  5m   ...     yeah  year  yearup  yesterday  \\\n",
       "0        0         1   0    0   0   ...        0     0       0          0   \n",
       "1        0         0   0    0   0   ...        0     0       0          0   \n",
       "2        0         0   0    0   0   ...        0     0       0          0   \n",
       "3        0         0   0    0   0   ...        0     0       0          0   \n",
       "4        0         0   0    0   0   ...        0     0       0          0   \n",
       "\n",
       "   york  youll  youre  yourselfshun  yourstoryco  youtube  \n",
       "0     0      0      0             0            0        0  \n",
       "1     0      0      0             0            0        0  \n",
       "2     0      0      0             0            0        0  \n",
       "3     0      0      0             0            0        0  \n",
       "4     0      0      0             0            0        0  \n",
       "\n",
       "[5 rows x 1153 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_coursera = normalize_corpus(df[0])\n",
    "bow_vectorizer_coursera = CountVectorizer()\n",
    "bow_coursera_corpus = bow_vectorizer_coursera.fit_transform(normalized_coursera)\n",
    "bow_feature_names_coursera = bow_vectorizer_coursera.get_feature_names()\n",
    "bow_table = pd.DataFrame(data = bow_coursera_corpus.todense(), columns = bow_feature_names_coursera)\n",
    "bow_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# set up the model \n",
    "no_topics = 5\n",
    "lda_coursera_corpus = LatentDirichletAllocation(n_components=no_topics, \n",
    "                                           doc_topic_prior =0.3,\n",
    "                                           topic_word_prior = 0.2).fit(bow_coursera_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_0</th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>word_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic_0</th>\n",
       "      <td>learn</td>\n",
       "      <td>online</td>\n",
       "      <td>startup</td>\n",
       "      <td>value</td>\n",
       "      <td>pick</td>\n",
       "      <td>series</td>\n",
       "      <td>round</td>\n",
       "      <td>fund</td>\n",
       "      <td>valuation</td>\n",
       "      <td>today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_1</th>\n",
       "      <td>course</td>\n",
       "      <td>free</td>\n",
       "      <td>astrobiology</td>\n",
       "      <td>cool</td>\n",
       "      <td>colleague</td>\n",
       "      <td>arizona</td>\n",
       "      <td>skill</td>\n",
       "      <td>take</td>\n",
       "      <td>data</td>\n",
       "      <td>mooc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_2</th>\n",
       "      <td>business</td>\n",
       "      <td>like</td>\n",
       "      <td>datascience</td>\n",
       "      <td>education</td>\n",
       "      <td>10kwomen</td>\n",
       "      <td>create</td>\n",
       "      <td>right</td>\n",
       "      <td>learning</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>lead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_3</th>\n",
       "      <td>first</td>\n",
       "      <td>time</td>\n",
       "      <td>campus</td>\n",
       "      <td>long</td>\n",
       "      <td>revision</td>\n",
       "      <td>overdue</td>\n",
       "      <td>wit</td>\n",
       "      <td>remember</td>\n",
       "      <td>maths</td>\n",
       "      <td>smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_4</th>\n",
       "      <td>online</td>\n",
       "      <td>raise</td>\n",
       "      <td>million</td>\n",
       "      <td>learner</td>\n",
       "      <td>fourth</td>\n",
       "      <td>prepare</td>\n",
       "      <td>revolution</td>\n",
       "      <td>industrial</td>\n",
       "      <td>billion</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word_0  word_1        word_2     word_3     word_4   word_5  \\\n",
       "Topic_0     learn  online       startup      value       pick   series   \n",
       "Topic_1    course    free  astrobiology       cool  colleague  arizona   \n",
       "Topic_2  business    like   datascience  education   10kwomen   create   \n",
       "Topic_3     first    time        campus       long   revision  overdue   \n",
       "Topic_4    online   raise       million    learner     fourth  prepare   \n",
       "\n",
       "             word_6      word_7        word_8     word_9  \n",
       "Topic_0       round        fund     valuation      today  \n",
       "Topic_1       skill        take          data       mooc  \n",
       "Topic_2       right    learning  entrepreneur       lead  \n",
       "Topic_3         wit    remember         maths      smith  \n",
       "Topic_4  revolution  industrial       billion  education  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Results\n",
    "no_top_words = 10\n",
    "def get_topic_words(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_words = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_word_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_words.append(keywords.take(top_word_locs).tolist())\n",
    "    return topic_words\n",
    "topic_words = get_topic_words(vectorizer = bow_vectorizer_coursera, \n",
    "                              lda_model = lda_coursera_corpus, \n",
    "                              n_words = no_top_words)\n",
    "pd.DataFrame(topic_words, \n",
    "             columns = [\"word_\" + str(i) for i in range(no_top_words)],\n",
    "             index = [\"Topic_\" + str(i) for i in range(len(topic_words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0m</th>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100daysofcode</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.008559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100daysofmlcode</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100daysofmlcodestudying</th>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.001427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100finished</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103mfor</th>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10kwomen</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.021138</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1m</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4th</th>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.001427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5m</th>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Topic_0   Topic_1   Topic_2   Topic_3   Topic_4\n",
       "0m                       0.000470  0.000084  0.000138  0.000164  0.000154\n",
       "100daysofcode            0.000090  0.000084  0.000140  0.000164  0.008559\n",
       "100daysofmlcode          0.000089  0.000125  0.003445  0.000164  0.000167\n",
       "100daysofmlcodestudying  0.000088  0.000084  0.000138  0.000164  0.001427\n",
       "100finished              0.000089  0.000084  0.001990  0.000164  0.000212\n",
       "103mfor                  0.000088  0.000084  0.000735  0.000163  0.000154\n",
       "10kwomen                 0.000089  0.000084  0.021138  0.000163  0.000175\n",
       "1m                       0.000089  0.000446  0.000138  0.000164  0.000155\n",
       "4th                      0.000088  0.000084  0.000138  0.000164  0.001427\n",
       "5m                       0.000088  0.000446  0.000138  0.000164  0.000155"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display a word vectors (words are in alphabetical order) for each topic\n",
    "word_weights = lda_coursera_corpus.components_ / lda_coursera_corpus.components_.sum(axis=1)[:, np.newaxis]\n",
    "word_weights_df = pd.DataFrame(word_weights.T, \n",
    "                               index = bow_feature_names_coursera, \n",
    "                               columns = [\"Topic_\" + str(i) for i in range(no_topics)])\n",
    "word_weights_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>learn</th>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online</th>\n",
       "      <td>0.110010</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.068615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startup</th>\n",
       "      <td>0.107584</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <td>0.086035</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pick</th>\n",
       "      <td>0.083858</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>series</th>\n",
       "      <td>0.023903</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.006976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>round</th>\n",
       "      <td>0.017082</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fund</th>\n",
       "      <td>0.014564</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valuation</th>\n",
       "      <td>0.013033</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.006249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.012404</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Topic_0   Topic_1   Topic_2   Topic_3   Topic_4\n",
       "learn      0.116600  0.000087  0.000151  0.000164  0.000200\n",
       "online     0.110010  0.000088  0.000174  0.000178  0.068615\n",
       "startup    0.107584  0.000084  0.000139  0.000164  0.000753\n",
       "value      0.086035  0.000084  0.000138  0.000164  0.000154\n",
       "pick       0.083858  0.000084  0.000141  0.000164  0.000154\n",
       "series     0.023903  0.000085  0.000140  0.000163  0.006976\n",
       "round      0.017082  0.000084  0.000139  0.000164  0.000773\n",
       "fund       0.014564  0.000084  0.000141  0.000164  0.000229\n",
       "valuation  0.013033  0.000084  0.000140  0.000164  0.006249\n",
       "today      0.012404  0.001058  0.000140  0.000164  0.000154"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort by word weights in Topic 0 (descending order) and see the weights by 10 most frequent words in Topic 0:\n",
    "word_weights_df.sort_values(by='Topic_0',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc_0</th>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.9109</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_1</th>\n",
       "      <td>0.0857</td>\n",
       "      <td>0.0857</td>\n",
       "      <td>0.6570</td>\n",
       "      <td>0.0857</td>\n",
       "      <td>0.0857</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_2</th>\n",
       "      <td>0.8153</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_3</th>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.7812</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic_0  Topic_1  Topic_2  Topic_3  Topic_4  dominant_topic\n",
       "Doc_0   0.0222   0.0222   0.9109   0.0222   0.0224               2\n",
       "Doc_1   0.0857   0.0857   0.6570   0.0857   0.0857               2\n",
       "Doc_2   0.8153   0.0462   0.0462   0.0462   0.0462               0\n",
       "Doc_3   0.0263   0.1401   0.0262   0.0263   0.7812               4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find out the dominant topic\n",
    "lda_coursera_output = lda_coursera_corpus.transform(bow_coursera_corpus)\n",
    "doc_names = [\"Doc_\" + str(i) for i in range(len(normalized_coursera))]\n",
    "topic_names = [\"Topic_\" + str(i) for i in range(no_topics)]\n",
    "df_document_topic = pd.DataFrame(np.round(lda_coursera_output, 4), columns=topic_names, index=doc_names)\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "df_document_topic[0:4]\n",
    "\n",
    "#find out which this belongs to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el515623128300767526302273303\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el515623128300767526302273303_data = {\"mdsDat\": {\"x\": [147.4699249267578, 129.96713256835938, -57.23094940185547, -66.37469482421875, 207.70989990234375], \"y\": [-163.06863403320312, 244.42013549804688, -75.24737548828125, 139.24166870117188, 43.632816314697266], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [24.481200590295966, 23.270401997843738, 22.03884270253004, 16.52834204020753, 13.681212669122722]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [225.0, 243.0, 179.0, 175.0, 314.0, 97.0, 94.0, 89.0, 87.0, 86.0, 86.0, 86.0, 86.0, 86.0, 85.0, 74.0, 99.0, 59.0, 54.0, 54.0, 53.0, 51.0, 58.0, 51.0, 165.0, 44.0, 43.0, 39.0, 73.0, 42.0, 179.16364246510267, 174.63273189016348, 43.76463242621485, 43.055454199033726, 41.783188091929965, 39.522510341165834, 222.38570098506486, 19.07918224769151, 11.641232933443217, 11.429605297669745, 11.558183292053856, 10.172112861593913, 7.928970072016489, 7.016700990510972, 6.60225263463246, 6.191444815270403, 6.392657959547996, 6.191712734614777, 5.327862337524827, 5.132599653135847, 5.132309461976831, 5.13243366073778, 4.8872509959061805, 4.887243061606428, 4.887417722677044, 4.308599448782478, 4.291214734470752, 4.290943560370024, 4.2215732645005275, 3.8968210576356523, 216.952123283994, 9.612196088444458, 209.4404185098631, 51.87078515983249, 13.30370164208845, 6.804491092886177, 70.50668394685492, 6.201405883089279, 7.942005693232332, 7.717240822507982, 6.055547402120751, 7.552894659485584, 50.87060340368497, 22.507935722168362, 15.771752940705655, 14.610893231822763, 14.290249917197343, 14.809063118672384, 14.017251273247078, 13.648257736912182, 14.093331239721964, 15.277930995940586, 12.874717516561077, 11.597643724560642, 11.594935375369499, 11.590931607449438, 11.140861094074126, 11.12365590091913, 11.222054877145325, 9.821718152703047, 9.4641485568809, 9.462521175994222, 11.34828069905455, 9.46241684269252, 9.46162132983673, 8.572307113121969, 8.541472364821347, 8.350935667620407, 7.864386645307027, 13.382461439389127, 7.674104293395117, 7.584731892257793, 8.180112735101204, 27.931782787650658, 12.398041675234628, 19.894574942795, 73.05763684249203, 17.551661871215444, 22.82216804168323, 15.305759135627774, 21.085557952944175, 14.73106187473675, 12.713217118664886, 22.948130760496813, 11.572682224729204, 58.619175748822144, 39.487762730894396, 36.65510800787519, 33.98281402394519, 34.75402460153998, 30.73383132827833, 28.84750172299512, 28.24837853607655, 29.080308414171274, 25.412203958620136, 22.833250167515832, 21.36761141026167, 19.89077660820058, 20.772591773712474, 18.835061250150144, 18.793364390648666, 19.164190469086932, 17.832305062281712, 17.832292396360227, 17.12714836688589, 33.3159697429739, 17.047039705984638, 17.1840882227481, 16.552490575296382, 13.670887307196022, 13.554188278717207, 10.937503022607032, 10.745792069840684, 10.372691850239882, 10.13073571184233, 31.964264160585678, 21.410265608461778, 20.363980876306123, 21.865047594979252, 19.493428513049718, 30.856610897191818, 29.695549508322845, 19.582749110481334, 26.32521147961814, 20.96415834238508, 21.27207220196814, 96.63224738197427, 94.07633236098935, 87.37425966032355, 86.51928216351104, 88.97174949605478, 86.51944068939973, 86.5195225905638, 86.51940355476576, 86.51957464099532, 85.49729789965744, 9.936236939504901, 4.659939438191037, 4.662392261765041, 4.166552084107311, 3.8512059701637598, 3.510595045452436, 3.4063216614663108, 3.2551204992987626, 2.839224435871392, 2.600943629257082, 2.5578012900847114, 2.5581682611104517, 2.5575780644350554, 2.557258469493682, 2.4036423994475045, 2.708772390579368, 2.3279765626395568, 2.3271040091405335, 2.188768394624378, 2.2668805243435575, 11.53882878845631, 3.113031612422968, 4.478315795511089, 4.1229288033943465, 2.8029456857514043, 3.5638413114611476, 2.5752406254852507, 2.5604347071966735, 58.722081954622546, 53.925159088786565, 53.92516089827629, 53.243214485212484, 51.03321987988667, 73.93307119194618, 18.976431339396395, 17.85575847222603, 12.689130412288785, 12.223947365461624, 11.34071487486389, 11.340673047660507, 9.481167734558833, 13.893229524650494, 9.120954517501723, 8.950863708915632, 7.279924103847047, 7.018869406424278, 6.646665732262771, 6.35501271183033, 5.945058612645439, 5.8899219612618845, 5.888785587258304, 4.131341429412414, 3.8844230036991307, 3.884939070605133, 3.8351405432019563, 5.223405996324325, 3.303261316727886, 3.7400930517351196, 3.738479906615864, 68.79987325813121, 6.826276407139959, 66.50148033252634, 7.257723724282145, 17.215044575406115, 9.482839164787642, 9.460884388945399], \"Term\": [\"startup\", \"learn\", \"value\", \"pick\", \"online\", \"first\", \"time\", \"campus\", \"long\", \"overdue\", \"remember\", \"revision\", \"wit\", \"maths\", \"smith\", \"million\", \"raise\", \"learner\", \"fourth\", \"prepare\", \"revolution\", \"industrial\", \"series\", \"datascience\", \"course\", \"astrobiology\", \"cool\", \"look\", \"free\", \"colleague\", \"value\", \"pick\", \"astrobiology\", \"cool\", \"colleague\", \"arizona\", \"startup\", \"techcrunch\", \"java\", \"third\", \"cloud\", \"spring\", \"give\", \"mark\", \"help\", \"dollar\", \"might\", \"pass\", \"performance\", \"scalable\", \"boot\", \"microservices\", \"networking\", \"nsx\", \"arm\", \"book\", \"classify\", \"image\", \"upskilling\", \"cour\", \"learn\", \"100daysofcode\", \"online\", \"free\", \"week\", \"tech\", \"course\", \"next\", \"start\", \"via\", \"news\", \"billion\", \"datascience\", \"science\", \"audit\", \"javascript\", \"thing\", \"worth\", \"folk\", \"unicorn\", \"provider\", \"th\", \"good\", \"mba\", \"improve\", \"unemployed\", \"innovation\", \"model\", \"analytics\", \"place\", \"node\", \"npm\", \"freetrial\", \"certification\", \"nodejs\", \"edtech\", \"another\", \"favorite\", \"certificate\", \"class\", \"push\", \"cod\", \"mit\", \"data\", \"set\", \"complete\", \"course\", \"take\", \"education\", \"skill\", \"free\", \"billion\", \"mooc\", \"online\", \"curriculum\", \"series\", \"look\", \"round\", \"world\", \"valuation\", \"fund\", \"learning\", \"today\", \"best\", \"create\", \"forward\", \"female\", \"announce\", \"seek\", \"impact\", \"janet\", \"funding\", \"effect\", \"ripple\", \"resource\", \"use\", \"excite\", \"well\", \"group\", \"develop\", \"machine\", \"north\", \"index\", \"tensorflow\", \"nabs\", \"like\", \"entrepreneur\", \"platform\", \"lead\", \"right\", \"business\", \"education\", \"10kwomen\", \"learn\", \"raise\", \"course\", \"first\", \"time\", \"long\", \"overdue\", \"campus\", \"remember\", \"revision\", \"wit\", \"maths\", \"smith\", \"thank\", \"expert\", \"hi\", \"ai\", \"job\", \"write\", \"udemy\", \"influencer\", \"thyself\", \"facebook\", \"tomorrow\", \"restart\", \"tonight\", \"criticalrole\", \"exactly\", \"sure\", \"several\", \"word\", \"issue\", \"dm\", \"know\", \"move\", \"finish\", \"want\", \"virtual\", \"great\", \"web\", \"dev\", \"learner\", \"fourth\", \"prepare\", \"revolution\", \"industrial\", \"million\", \"woman\", \"grow\", \"component\", \"india\", \"auto\", \"shweta\", \"catalyst\", \"community\", \"success\", \"program\", \"launch\", \"path\", \"teach\", \"way\", \"owner\", \"sustainable\", \"food\", \"fellowship\", \"cursos\", \"formacion\", \"select\", \"growth\", \"entire\", \"dare\", \"around\", \"raise\", \"access\", \"online\", \"company\", \"business\", \"curriculum\", \"10kwomen\"], \"Total\": [225.0, 243.0, 179.0, 175.0, 314.0, 97.0, 94.0, 89.0, 87.0, 86.0, 86.0, 86.0, 86.0, 86.0, 85.0, 74.0, 99.0, 59.0, 54.0, 54.0, 53.0, 51.0, 58.0, 51.0, 165.0, 44.0, 43.0, 39.0, 73.0, 42.0, 179.55120918814325, 175.01912767048066, 44.14584366450302, 43.43693466423954, 42.16561361467124, 39.90346203512213, 225.19845970105422, 19.459512012850283, 12.022760050387557, 11.810442507180973, 11.969968264357986, 10.552669371263722, 8.327034775477982, 7.4057319426067085, 6.988344165228423, 6.571869751489303, 6.7856981952095765, 6.572437116521232, 5.711550044542708, 5.513167653195034, 5.513089193147059, 5.513914434932973, 5.267640305559521, 5.267946224069533, 5.268140094529543, 4.692258303821086, 4.675163259491408, 4.675675799975329, 4.6117274877552275, 4.2778910665046554, 243.6455870510034, 10.582198502207723, 314.24680694535846, 73.25086953990066, 17.268084531790613, 8.035133375697717, 165.56341398240295, 8.33609156451882, 15.029995595566392, 15.300546001924417, 9.826961865089137, 37.47128270036182, 51.31290755094543, 22.91228148112866, 16.158948100534587, 15.003210808995037, 14.683616859084625, 15.21728547407386, 14.404667441336384, 14.036354223816963, 14.49444127688671, 15.71757320095775, 13.265659589378023, 11.98858373928979, 11.989510651864146, 11.989659183608499, 11.528130328735623, 11.516329227539531, 11.62193346862269, 10.212001594971161, 9.85697644179267, 9.856829296003331, 11.821759630912819, 9.857769073807498, 9.857742576917428, 8.958479938082634, 8.927369362670808, 8.740803902053173, 8.252906036499155, 14.056319746187688, 8.064018354791987, 7.980480592668403, 8.669040218075788, 33.71303086566792, 13.89817243324701, 24.766227655262124, 165.56341398240295, 33.32043240769974, 56.367084240383214, 30.97544242531086, 73.25086953990066, 37.47128270036182, 25.37673494021001, 314.24680694535846, 21.37477912597103, 58.99948242005984, 39.888401348111046, 37.08166639341328, 34.37832491306018, 35.17798474656464, 31.1136472286253, 29.23190408936282, 28.631089578265176, 29.511232081164867, 25.799602497621173, 23.213316671829126, 21.75061782364771, 20.27489898090274, 21.178774510159165, 19.217201450449096, 19.175332666738427, 19.562943499356102, 18.21414382010165, 18.21433983706428, 17.507261648202444, 34.062109656731636, 17.432147576531424, 17.591707579074207, 16.958293055266253, 14.055290464188552, 13.93699733027852, 11.317531072372788, 11.126385477904385, 10.753394047273545, 10.510344417281035, 36.78547000490737, 24.653720210951054, 24.38965421306104, 27.944015079835452, 24.768982724440967, 57.171560560661916, 56.367084240383214, 29.330201267289514, 243.6455870510034, 99.7950275169894, 165.56341398240295, 97.0159211004467, 94.46088901185807, 87.75649949470186, 86.89792133693034, 89.3612010631698, 86.89828156801184, 86.89846062043908, 86.8983932542034, 86.89893830622212, 85.87592903834516, 10.335267272863826, 5.0421098882884525, 5.0536480485491415, 4.544637790703815, 4.234105600451252, 3.8872696057472766, 3.799743536481621, 3.6722358474817, 3.212614099135857, 2.9750664879501176, 2.936359511883157, 2.9368513870786166, 2.936398497258026, 2.9367610217572095, 2.7804874743308763, 3.135137733292881, 2.7020387327703688, 2.702074394843896, 2.562965429634431, 2.663133053131168, 14.198617864493123, 5.431943996077586, 13.16164499178447, 14.866042166732775, 4.716405117068833, 14.240496245291986, 8.660809089629472, 3.9008395564010305, 59.10092885778904, 54.30329657693962, 54.30334850066244, 53.62144019287608, 51.41089978792291, 74.7780127176169, 19.356275878070633, 18.232329576775932, 13.065174060406402, 12.599546608870515, 11.716060799810116, 11.716331150888038, 9.857541549639459, 14.466180489104152, 9.50923695499169, 9.347534116518965, 7.6625807503695285, 7.408063661511978, 7.029446250152317, 6.732318176921372, 6.321128210813294, 6.265545759591168, 6.265683421900434, 4.511963147083488, 4.259711183246872, 4.2605180377167295, 4.213165302002079, 5.758230943726604, 3.6796934220624773, 4.173847390426337, 4.174282684932782, 99.7950275169894, 8.361053946167067, 314.24680694535846, 11.716533955671256, 57.171560560661916, 21.37477912597103, 29.330201267289514], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.4051, 1.4051, 1.3986, 1.3984, 1.3982, 1.3977, 1.3947, 1.3875, 1.375, 1.3745, 1.3723, 1.3705, 1.3583, 1.3533, 1.3504, 1.3476, 1.3476, 1.3476, 1.3377, 1.3357, 1.3357, 1.3356, 1.3323, 1.3323, 1.3323, 1.322, 1.3216, 1.3214, 1.3189, 1.314, 1.2912, 1.3111, 1.0015, 1.0621, 1.1464, 1.241, 0.5536, 1.1114, 0.7694, 0.7228, 0.9231, -0.1944, 1.4493, 1.4402, 1.4337, 1.4315, 1.4308, 1.4308, 1.4307, 1.4299, 1.4299, 1.4296, 1.4281, 1.4248, 1.4245, 1.4242, 1.4238, 1.4233, 1.423, 1.419, 1.4173, 1.4172, 1.4171, 1.4171, 1.417, 1.4139, 1.4138, 1.4124, 1.4098, 1.4089, 1.4084, 1.4071, 1.3999, 1.2699, 1.3438, 1.239, 0.6399, 0.817, 0.5538, 0.753, 0.2127, 0.5244, 0.7668, -1.159, 0.8444, 1.5059, 1.5023, 1.5008, 1.5008, 1.5002, 1.5001, 1.4991, 1.4989, 1.4977, 1.4972, 1.4959, 1.4946, 1.4932, 1.493, 1.4923, 1.4922, 1.4918, 1.4912, 1.4912, 1.4904, 1.4902, 1.49, 1.4889, 1.4881, 1.4846, 1.4845, 1.4782, 1.4776, 1.4763, 1.4756, 1.3719, 1.3713, 1.332, 1.2671, 1.2728, 0.8957, 0.8715, 1.1084, -0.7128, -0.0479, -0.5396, 1.7961, 1.796, 1.7957, 1.7957, 1.7957, 1.7957, 1.7957, 1.7957, 1.7957, 1.7957, 1.7607, 1.7213, 1.7195, 1.7132, 1.7053, 1.6982, 1.6908, 1.6795, 1.6765, 1.6657, 1.6621, 1.662, 1.662, 1.6617, 1.6545, 1.6539, 1.6511, 1.6507, 1.6423, 1.639, 1.5927, 1.2434, 0.722, 0.5176, 1.2797, 0.4148, 0.5872, 1.3791, 1.9827, 1.9822, 1.9822, 1.9821, 1.9818, 1.9778, 1.9693, 1.9683, 1.9599, 1.9589, 1.9566, 1.9566, 1.9502, 1.9487, 1.9475, 1.9458, 1.9379, 1.9352, 1.9332, 1.9315, 1.9278, 1.9273, 1.9271, 1.901, 1.8969, 1.8969, 1.8951, 1.8917, 1.8812, 1.8794, 1.8789, 1.6172, 1.7863, 0.4362, 1.5102, 0.7889, 1.1764, 0.8577], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.325, -2.3506, -3.7344, -3.7508, -3.7808, -3.8364, -2.1088, -4.5647, -5.0587, -5.0771, -5.0659, -5.1936, -5.4427, -5.565, -5.6258, -5.6901, -5.6581, -5.69, -5.8403, -5.8776, -5.8777, -5.8777, -5.9266, -5.9266, -5.9266, -6.0526, -6.0567, -6.0568, -6.0731, -6.1531, -2.1336, -5.2502, -2.1688, -3.5645, -4.9252, -5.5957, -3.2576, -5.6885, -5.4411, -5.4698, -5.7123, -5.4913, -3.5332, -4.3487, -4.7043, -4.7808, -4.803, -4.7673, -4.8222, -4.8489, -4.8168, -4.7361, -4.9073, -5.0117, -5.012, -5.0123, -5.0519, -5.0535, -5.0447, -5.1779, -5.215, -5.2152, -5.0335, -5.2152, -5.2153, -5.314, -5.3176, -5.3402, -5.4002, -4.8686, -5.4247, -5.4364, -5.3608, -4.1328, -4.945, -4.4721, -3.1713, -4.5974, -4.3348, -4.7343, -4.4139, -4.7726, -4.9199, -4.3293, -5.0139, -3.3371, -3.7322, -3.8066, -3.8823, -3.8599, -3.9828, -4.0461, -4.0671, -4.0381, -4.1729, -4.2799, -4.3463, -4.4179, -4.3745, -4.4724, -4.4747, -4.4551, -4.5271, -4.5271, -4.5675, -3.9021, -4.5722, -4.5642, -4.6016, -4.7929, -4.8015, -5.016, -5.0336, -5.069, -5.0926, -3.9435, -4.3443, -4.3944, -4.3233, -4.4381, -3.9788, -4.0172, -4.4335, -4.1376, -4.3653, -4.3508, -2.5495, -2.5763, -2.6502, -2.6601, -2.6321, -2.6601, -2.6601, -2.6601, -2.6601, -2.6719, -4.8242, -5.5814, -5.5809, -5.6933, -5.772, -5.8646, -5.8948, -5.9402, -6.0769, -6.1646, -6.1813, -6.1811, -6.1814, -6.1815, -6.2434, -6.1239, -6.2754, -6.2758, -6.3371, -6.302, -4.6747, -5.9848, -5.6212, -5.7039, -6.0898, -5.8496, -6.1745, -6.1803, -2.8586, -2.9438, -2.9438, -2.9565, -2.9989, -2.6282, -3.9882, -4.049, -4.3906, -4.428, -4.503, -4.503, -4.6821, -4.3, -4.7208, -4.7396, -4.9463, -4.9828, -5.0373, -5.0821, -5.1488, -5.1581, -5.1583, -5.5128, -5.5744, -5.5743, -5.5872, -5.2782, -5.7365, -5.6123, -5.6127, -2.7002, -5.0106, -2.7342, -4.9493, -4.0856, -4.6819, -4.6842]}, \"token.table\": {\"Topic\": [1, 4, 3, 5, 3, 5, 4, 2, 3, 2, 1, 1, 5, 1, 2, 5, 3, 1, 2, 3, 1, 1, 2, 3, 5, 4, 5, 2, 2, 2, 1, 1, 2, 1, 5, 3, 5, 2, 3, 4, 5, 1, 1, 1, 2, 3, 4, 3, 4, 2, 5, 5, 5, 1, 2, 2, 4, 5, 3, 4, 1, 2, 2, 3, 5, 3, 5, 1, 3, 4, 3, 4, 4, 2, 5, 3, 3, 4, 4, 2, 5, 5, 3, 5, 1, 2, 2, 3, 3, 1, 2, 2, 4, 3, 5, 5, 1, 4, 1, 3, 2, 3, 5, 5, 4, 2, 4, 3, 1, 2, 4, 2, 4, 5, 2, 3, 5, 1, 3, 5, 3, 2, 3, 4, 3, 3, 1, 4, 2, 1, 1, 3, 5, 2, 2, 1, 2, 3, 1, 2, 4, 3, 1, 1, 2, 5, 1, 2, 2, 2, 3, 2, 1, 1, 2, 3, 5, 4, 5, 1, 5, 1, 1, 2, 2, 3, 5, 5, 5, 2, 2, 2, 3, 5, 4, 3, 4, 4, 5, 2, 3, 3, 3, 1, 2, 3, 5, 3, 2, 3, 4, 5, 2, 3, 4, 1, 1, 2, 1, 3, 5, 4, 5, 1, 2, 3, 5, 1, 5, 1, 3, 2, 4, 2, 1, 4, 4, 3, 4, 4, 4, 2, 2, 1, 3, 3, 1, 1, 2, 5, 4, 5, 2, 4, 5, 2, 4, 1, 3, 3, 4, 5, 4, 3, 2, 4], \"Freq\": [0.9449832185546074, 0.09449832185546074, 0.6818909907142364, 0.30685094582140643, 0.11960214662392259, 0.837215026367458, 0.8801581521374735, 0.9464862305138979, 0.9864414130417285, 1.0081357267049902, 1.0024192879503262, 0.9491015634136266, 0.9582484709140898, 0.9966963217282381, 0.9901634624020279, 0.9388821198485313, 0.9826766947662903, 0.21349682806355472, 0.40030655261916515, 0.40030655261916515, 0.8524679889729528, 0.9069325426868035, 0.15742092592435963, 0.5422276337394609, 0.29735063785712373, 0.9959579654383286, 0.9130065498257196, 0.9693555172710488, 0.9129854769993927, 0.924850902280152, 0.8555850946764891, 1.0025089235809788, 1.002445893715916, 0.9960723063066343, 0.9677744592323263, 0.3413978925110224, 0.5974463118942893, 0.8075513266854173, 0.1211326990028126, 0.08075513266854173, 0.9950116194315458, 0.9899409415600577, 0.9350401723221736, 0.4288387047125417, 0.4409186682255711, 0.12683961688680812, 0.006039981756514672, 0.9690071776224111, 1.021533579945484, 0.5614093099759624, 0.42105698248197176, 0.9390307999593266, 0.9583484075567557, 0.148310604879249, 0.8305393873237943, 0.9939019719232483, 0.7690652118919349, 0.256355070630645, 0.9960662168932456, 0.7509951474818384, 0.9129821842011236, 1.0046347217613183, 0.4080395555305671, 0.5322255072137831, 0.07096340096183776, 0.9882429927963281, 0.8152853120895307, 0.12168548901870865, 0.8517984231309605, 0.7192983311249405, 0.9752097339335736, 0.9916483596705691, 1.0083808251515958, 0.9152476236334324, 0.8865320636729006, 0.9654898159797731, 0.6078267575970647, 0.30391337879853236, 0.999835891879744, 0.9719072000110791, 0.9575970562170775, 0.9388529668433595, 0.9908105905396966, 0.9944147667626422, 0.7098891839321436, 0.2866860165879811, 0.9304875368330114, 0.9963473511224765, 0.9712239878740829, 0.9607261427031558, 0.9799738876466617, 0.702222719472018, 0.2808890877888072, 1.0024593834177664, 0.9872572741844318, 0.8683222414771901, 1.0016678964996562, 0.98938429268644, 0.8554913067371152, 0.9886975504206925, 1.000874877085515, 0.9886409222334269, 0.9524152235407889, 0.9920075355689566, 0.8169409930621157, 0.9541876858019919, 0.7803460697810779, 0.9908563428971139, 0.9981069196846507, 0.999785991876278, 0.9447095508373003, 0.14085878069875066, 0.8451526841925039, 0.9135303402397977, 0.035785838117501065, 0.7872884385850234, 0.1789291905875053, 0.8906379246449247, 0.10671237806805549, 0.9982922627488325, 0.9920667470495975, 0.13592323271479131, 0.8699086893746645, 0.9913795616386507, 0.9777278276871049, 1.0045205339592485, 0.9452137957799351, 1.0011629796145696, 1.0009522609975017, 0.9067968063346961, 0.8842126229892973, 0.013372914893797527, 0.989595702141017, 0.9228241880017152, 0.955165468324333, 0.0788123454302608, 0.5122802452966952, 0.394061727151304, 0.18409615429063725, 0.18409615429063725, 0.5522884628719118, 0.9514436066964723, 0.9491916133155389, 0.610565104695822, 0.305282552347911, 0.10176085078263701, 0.7197617676775524, 0.2399205892258508, 0.9130588931754804, 0.9129879310374882, 0.9719434326848978, 0.9130725236003883, 0.9491364921598341, 0.6650823345878615, 0.07319087892593691, 0.04773318190821972, 0.21320821252338143, 1.001174696258543, 0.9491976431890825, 0.9129033710977184, 0.9449162857992108, 0.8754191000702896, 0.9998907109712222, 0.9792399567313463, 0.08200198258362223, 0.8200198258362223, 0.08200198258362223, 0.9944138159241002, 0.9628207704634314, 0.9658875242279839, 0.9920612339933547, 0.10020539348313291, 0.21043132631457911, 0.6914172150336171, 1.0011705459550262, 0.9710256430505494, 1.0215021479122917, 1.0011684830644403, 0.9884106023515825, 0.20186537556369705, 0.7670884271420488, 0.9882323576379024, 0.9977976611798712, 0.9069196357746097, 1.0038284497745713, 0.9915587887262595, 0.9494049516878001, 1.0000087726183167, 0.8634228750316675, 0.07195190625263896, 0.7401818396398128, 0.9388604554051253, 0.48425458445568803, 0.48425458445568803, 0.9898000633221209, 0.9476275289389137, 0.5322689517194451, 0.46573533275451445, 0.9857971510759883, 0.01332158312264849, 0.9464481790282472, 0.9568957587228731, 0.9576180958881872, 0.09003484598557475, 0.5402090759134486, 0.3901509992708239, 0.9958110142528398, 0.8711740891783476, 0.12445344131119251, 0.9763862520012403, 0.9299389528588359, 0.9543458018751887, 0.9675608512085507, 0.953443564644519, 0.9313791581739458, 0.9338189734045409, 0.9951208482507483, 0.9779578916638836, 1.0216732616899586, 1.0216596973474017, 0.7895269697011854, 1.0008624779264483, 0.9974099952710458, 0.8673539385448407, 0.968818441739655, 0.9949404507436425, 0.9969300725367677, 0.5228571581036262, 0.3921428685777196, 0.13071428952590655, 0.6360776747406401, 0.4240517831604268, 0.6726739967398987, 0.2690695986959595, 0.8912234749344163, 0.6927759217305057, 0.34638796086525286, 0.7528339333796373, 0.23164121027065762, 0.9663644034318727, 1.0011692592002175, 0.981593779696317, 0.7401720706936878, 0.98899524877908, 0.9857211409719522, 1.0289998908452485], \"Term\": [\"100daysofcode\", \"100daysofcode\", \"10kwomen\", \"10kwomen\", \"access\", \"access\", \"ai\", \"analytics\", \"announce\", \"another\", \"arizona\", \"arm\", \"around\", \"astrobiology\", \"audit\", \"auto\", \"best\", \"billion\", \"billion\", \"billion\", \"book\", \"boot\", \"business\", \"business\", \"business\", \"campus\", \"catalyst\", \"certificate\", \"certification\", \"class\", \"classify\", \"cloud\", \"cod\", \"colleague\", \"community\", \"company\", \"company\", \"complete\", \"complete\", \"complete\", \"component\", \"cool\", \"cour\", \"course\", \"course\", \"course\", \"course\", \"create\", \"criticalrole\", \"curriculum\", \"curriculum\", \"cursos\", \"dare\", \"data\", \"data\", \"datascience\", \"dev\", \"dev\", \"develop\", \"dm\", \"dollar\", \"edtech\", \"education\", \"education\", \"education\", \"effect\", \"entire\", \"entrepreneur\", \"entrepreneur\", \"exactly\", \"excite\", \"expert\", \"facebook\", \"favorite\", \"fellowship\", \"female\", \"finish\", \"finish\", \"first\", \"folk\", \"food\", \"formacion\", \"forward\", \"fourth\", \"free\", \"free\", \"freetrial\", \"fund\", \"funding\", \"give\", \"good\", \"great\", \"great\", \"group\", \"grow\", \"growth\", \"help\", \"hi\", \"image\", \"impact\", \"improve\", \"index\", \"india\", \"industrial\", \"influencer\", \"innovation\", \"issue\", \"janet\", \"java\", \"javascript\", \"job\", \"know\", \"know\", \"launch\", \"lead\", \"lead\", \"lead\", \"learn\", \"learn\", \"learner\", \"learning\", \"like\", \"like\", \"long\", \"look\", \"machine\", \"mark\", \"maths\", \"mba\", \"microservices\", \"might\", \"million\", \"million\", \"mit\", \"model\", \"mooc\", \"mooc\", \"mooc\", \"move\", \"move\", \"move\", \"nabs\", \"networking\", \"news\", \"news\", \"news\", \"next\", \"next\", \"node\", \"nodejs\", \"north\", \"npm\", \"nsx\", \"online\", \"online\", \"online\", \"online\", \"overdue\", \"owner\", \"pass\", \"path\", \"performance\", \"pick\", \"place\", \"platform\", \"platform\", \"platform\", \"prepare\", \"program\", \"provider\", \"push\", \"raise\", \"raise\", \"raise\", \"remember\", \"resource\", \"restart\", \"revision\", \"revolution\", \"right\", \"right\", \"ripple\", \"round\", \"scalable\", \"science\", \"seek\", \"select\", \"series\", \"set\", \"set\", \"several\", \"shweta\", \"skill\", \"skill\", \"smith\", \"spring\", \"start\", \"start\", \"startup\", \"startup\", \"success\", \"sure\", \"sustainable\", \"take\", \"take\", \"take\", \"teach\", \"tech\", \"tech\", \"techcrunch\", \"tensorflow\", \"th\", \"thank\", \"thing\", \"third\", \"thyself\", \"time\", \"today\", \"tomorrow\", \"tonight\", \"udemy\", \"unemployed\", \"unicorn\", \"upskilling\", \"use\", \"valuation\", \"value\", \"via\", \"via\", \"via\", \"virtual\", \"virtual\", \"want\", \"want\", \"way\", \"web\", \"web\", \"week\", \"week\", \"well\", \"wit\", \"woman\", \"word\", \"world\", \"worth\", \"write\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 4, 1, 5, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el515623128300767526302273303\", ldavis_el515623128300767526302273303_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el515623128300767526302273303\", ldavis_el515623128300767526302273303_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el515623128300767526302273303\", ldavis_el515623128300767526302273303_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=                x           y  topics  cluster       Freq\n",
       "topic                                                    \n",
       "1      147.469925 -163.068634       1        1  24.481201\n",
       "3      129.967133  244.420135       2        1  23.270402\n",
       "0      -57.230949  -75.247375       3        1  22.038843\n",
       "4      -66.374695  139.241669       4        1  16.528342\n",
       "2      207.709900   43.632816       5        1  13.681213, topic_info=     Category        Freq          Term       Total  loglift  logprob\n",
       "term                                                                 \n",
       "954   Default  225.000000       startup  225.000000  30.0000  30.0000\n",
       "584   Default  243.000000         learn  243.000000  29.0000  29.0000\n",
       "1090  Default  179.000000         value  179.000000  28.0000  28.0000\n",
       "771   Default  175.000000          pick  175.000000  27.0000  27.0000\n",
       "729   Default  314.000000        online  314.000000  26.0000  26.0000\n",
       "386   Default   97.000000         first   97.000000  25.0000  25.0000\n",
       "1041  Default   94.000000          time   94.000000  24.0000  24.0000\n",
       "130   Default   89.000000        campus   89.000000  23.0000  23.0000\n",
       "608   Default   87.000000          long   87.000000  22.0000  22.0000\n",
       "747   Default   86.000000       overdue   86.000000  21.0000  21.0000\n",
       "842   Default   86.000000      remember   86.000000  20.0000  20.0000\n",
       "859   Default   86.000000      revision   86.000000  19.0000  19.0000\n",
       "1129  Default   86.000000           wit   86.000000  18.0000  18.0000\n",
       "636   Default   86.000000         maths   86.000000  17.0000  17.0000\n",
       "922   Default   85.000000         smith   85.000000  16.0000  16.0000\n",
       "654   Default   74.000000       million   74.000000  15.0000  15.0000\n",
       "823   Default   99.000000         raise   99.000000  14.0000  14.0000\n",
       "585   Default   59.000000       learner   59.000000  13.0000  13.0000\n",
       "401   Default   54.000000        fourth   54.000000  12.0000  12.0000\n",
       "790   Default   54.000000       prepare   54.000000  11.0000  11.0000\n",
       "861   Default   53.000000    revolution   53.000000  10.0000  10.0000\n",
       "499   Default   51.000000    industrial   51.000000   9.0000   9.0000\n",
       "896   Default   58.000000        series   58.000000   8.0000   8.0000\n",
       "247   Default   51.000000   datascience   51.000000   7.0000   7.0000\n",
       "216   Default  165.000000        course  165.000000   6.0000   6.0000\n",
       "77    Default   44.000000  astrobiology   44.000000   5.0000   5.0000\n",
       "206   Default   43.000000          cool   43.000000   4.0000   4.0000\n",
       "609   Default   39.000000          look   39.000000   3.0000   3.0000\n",
       "404   Default   73.000000          free   73.000000   2.0000   2.0000\n",
       "180   Default   42.000000     colleague   42.000000   1.0000   1.0000\n",
       "...       ...         ...           ...         ...      ...      ...\n",
       "189    Topic5   12.689130     component   13.065174   1.9599  -4.3906\n",
       "498    Topic5   12.223947         india   12.599547   1.9589  -4.4280\n",
       "83     Topic5   11.340715          auto   11.716061   1.9566  -4.5030\n",
       "908    Topic5   11.340673        shweta   11.716331   1.9566  -4.5030\n",
       "142    Topic5    9.481168      catalyst    9.857542   1.9502  -4.6821\n",
       "184    Topic5   13.893230     community   14.466180   1.9487  -4.3000\n",
       "987    Topic5    9.120955       success    9.509237   1.9475  -4.7208\n",
       "802    Topic5    8.950864       program    9.347534   1.9458  -4.7396\n",
       "576    Topic5    7.279924        launch    7.662581   1.9379  -4.9463\n",
       "759    Topic5    7.018869          path    7.408064   1.9352  -4.9828\n",
       "1011   Topic5    6.646666         teach    7.029446   1.9332  -5.0373\n",
       "1116   Topic5    6.355013           way    6.732318   1.9315  -5.0821\n",
       "751    Topic5    5.945059         owner    6.321128   1.9278  -5.1488\n",
       "997    Topic5    5.889922   sustainable    6.265546   1.9273  -5.1581\n",
       "393    Topic5    5.888786          food    6.265683   1.9271  -5.1583\n",
       "377    Topic5    4.131341    fellowship    4.511963   1.9010  -5.5128\n",
       "237    Topic5    3.884423        cursos    4.259711   1.8969  -5.5744\n",
       "397    Topic5    3.884939     formacion    4.260518   1.8969  -5.5743\n",
       "891    Topic5    3.835141        select    4.213165   1.8951  -5.5872\n",
       "453    Topic5    5.223406        growth    5.758231   1.8917  -5.2782\n",
       "330    Topic5    3.303261        entire    3.679693   1.8812  -5.7365\n",
       "241    Topic5    3.740093          dare    4.173847   1.8794  -5.6123\n",
       "69     Topic5    3.738480        around    4.174283   1.8789  -5.6127\n",
       "823    Topic5   68.799873         raise   99.795028   1.6172  -2.7002\n",
       "15     Topic5    6.826276        access    8.361054   1.7863  -5.0106\n",
       "729    Topic5   66.501480        online  314.246807   0.4362  -2.7342\n",
       "186    Topic5    7.257724       company   11.716534   1.5102  -4.9493\n",
       "125    Topic5   17.215045      business   57.171561   0.7889  -4.0856\n",
       "236    Topic5    9.482839    curriculum   21.374779   1.1764  -4.6819\n",
       "6      Topic5    9.460884      10kwomen   29.330201   0.8577  -4.6842\n",
       "\n",
       "[232 rows x 6 columns], token_table=      Topic      Freq           Term\n",
       "term                                \n",
       "1         1  0.944983  100daysofcode\n",
       "1         4  0.094498  100daysofcode\n",
       "6         3  0.681891       10kwomen\n",
       "6         5  0.306851       10kwomen\n",
       "15        3  0.119602         access\n",
       "15        5  0.837215         access\n",
       "32        4  0.880158             ai\n",
       "48        2  0.946486      analytics\n",
       "54        3  0.986441       announce\n",
       "56        2  1.008136        another\n",
       "67        1  1.002419        arizona\n",
       "68        1  0.949102            arm\n",
       "69        5  0.958248         around\n",
       "77        1  0.996696   astrobiology\n",
       "81        2  0.990163          audit\n",
       "83        5  0.938882           auto\n",
       "103       3  0.982677           best\n",
       "108       1  0.213497        billion\n",
       "108       2  0.400307        billion\n",
       "108       3  0.400307        billion\n",
       "113       1  0.852468           book\n",
       "115       1  0.906933           boot\n",
       "125       2  0.157421       business\n",
       "125       3  0.542228       business\n",
       "125       5  0.297351       business\n",
       "130       4  0.995958         campus\n",
       "142       5  0.913007       catalyst\n",
       "150       2  0.969356    certificate\n",
       "152       2  0.912985  certification\n",
       "166       2  0.924851          class\n",
       "...     ...       ...            ...\n",
       "1041      4  0.995121           time\n",
       "1042      3  0.977958          today\n",
       "1043      4  1.021673       tomorrow\n",
       "1045      4  1.021660        tonight\n",
       "1068      4  0.789527          udemy\n",
       "1070      2  1.000862     unemployed\n",
       "1072      2  0.997410        unicorn\n",
       "1079      1  0.867354     upskilling\n",
       "1082      3  0.968818            use\n",
       "1089      3  0.994940      valuation\n",
       "1090      1  0.996930          value\n",
       "1096      1  0.522857            via\n",
       "1096      2  0.392143            via\n",
       "1096      5  0.130714            via\n",
       "1100      4  0.636078        virtual\n",
       "1100      5  0.424052        virtual\n",
       "1112      2  0.672674           want\n",
       "1112      4  0.269070           want\n",
       "1116      5  0.891223            way\n",
       "1117      2  0.692776            web\n",
       "1117      4  0.346388            web\n",
       "1120      1  0.752834           week\n",
       "1120      3  0.231641           week\n",
       "1121      3  0.966364           well\n",
       "1129      4  1.001169            wit\n",
       "1133      5  0.981594          woman\n",
       "1134      4  0.740172           word\n",
       "1136      3  0.988995          world\n",
       "1138      2  0.985721          worth\n",
       "1140      4  1.029000          write\n",
       "\n",
       "[246 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 4, 1, 5, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "visualization_panel = pyLDAvis.sklearn.prepare(lda_coursera_corpus, bow_coursera_corpus, bow_vectorizer_coursera, mds='tsne')\n",
    "visualization_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** LOG-LIKELIHOOD, PERPLEXITY AND COHERENCE SCORES ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "C:\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "coursera_corpus_tokenized = [tokenize_text(normalized_coursera[doc_id]) for doc_id in range(len(normalized_coursera))]\n",
    "coursera_dictionary = Dictionary(coursera_corpus_tokenized)\n",
    "coursera_corpus_bow = [coursera_dictionary.doc2bow(doc) for doc in coursera_corpus_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score for the model:  -6.6168\n",
      "Coherence score by topic:  [ -4.8323  -0.0441  -7.1457 -10.928  -10.134 ]\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(topics=topic_words, \n",
    "                    corpus = coursera_corpus_bow , \n",
    "                    dictionary = coursera_dictionary, coherence='u_mass')\n",
    "print(\"Coherence score for the model: \", np.round(cm.get_coherence(),4))  # get coherence value\n",
    "print(\"Coherence score by topic: \", np.round(cm.get_coherence_per_topic(),4)) #coherence score by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -43179.688970068324\n"
     ]
    }
   ],
   "source": [
    "print(\"Log-Likelihood (higher values are better): \", lda_coursera_corpus.score(bow_coursera_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (lower values are better):  222.64297410156001\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity (lower values are better): \", lda_coursera_corpus.perplexity(bow_coursera_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_highest_loglikelihood(lowest, highest):\n",
    "    candidate = []\n",
    "    for i in range(lowest,highest+1):\n",
    "        candidate.append(i)\n",
    "    no_keywords = 10\n",
    "    res = []\n",
    "    for i in range(lowest,highest+1):\n",
    "        no_topics = i    \n",
    "        lda_coursera_corpus = LatentDirichletAllocation(n_components=no_topics, max_iter=100,random_state = 42).fit(bow_coursera_corpus)\n",
    "        res.append(lda_coursera_corpus.score(bow_coursera_corpus))\n",
    "    idx = res.index(max(res))\n",
    "    return candidate[idx]\n",
    "def select_lowest_perplexity(lowest, highest):\n",
    "    candidate = []\n",
    "    for i in range(lowest,highest+1):\n",
    "        candidate.append(i)\n",
    "    no_keywords = 10\n",
    "    res = []\n",
    "    for i in range(lowest,highest+1):\n",
    "        no_topics = i    \n",
    "        lda_coursera_corpus = LatentDirichletAllocation(n_components=no_topics, max_iter=100,random_state = 42).fit(bow_coursera_corpus)\n",
    "        res.append(lda_coursera_corpus.perplexity(bow_coursera_corpus))\n",
    "    idx = res.index(min(res))\n",
    "    return candidate[idx]\n",
    "def select_highest_coherence_score(lowest, highest):\n",
    "    candidate = []\n",
    "    for i in range(lowest,highest+1):\n",
    "        candidate.append(i)\n",
    "    no_keywords = 10\n",
    "    res = []\n",
    "    for i in range(lowest,highest+1):\n",
    "        no_topics = i    \n",
    "        lda_coursera_corpus = LatentDirichletAllocation(n_components=no_topics, max_iter=100,random_state = 42).fit(bow_coursera_corpus)\n",
    "        topic_words = get_topic_words(vectorizer = bow_vectorizer_coursera, \n",
    "                              lda_model = lda_coursera_corpus, \n",
    "                              n_words = no_keywords)\n",
    "        cm = CoherenceModel(topics=topic_words, corpus = coursera_corpus_bow , dictionary = coursera_dictionary, coherence='u_mass')\n",
    "        res.append(np.round(cm.get_coherence(),4))\n",
    "    idx = res.index(max(res))\n",
    "    return candidate[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_highest_loglikelihood(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_lowest_perplexity(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, [-11.9828, -10.4517, -9.6755, -8.359])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_highest_coherence_score(2, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
